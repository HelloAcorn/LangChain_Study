import streamlit as st
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader
from st_chat_message import message
from PIL import Image
import base64
import io
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from dotenv import load_dotenv
from dotenv import load_dotenv
import os
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    SystemMessage,
    HumanMessage,
    AIMessage)
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import json

# ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ
image_path = "/Users/icarus/Library/CloudStorage/OneDrive-koreatech.ac.kr/Python/solo_study/project/grad_sup/ph.jpeg"

# ì´ë¯¸ì§€ ë¡œë“œ ë° ì¸ì½”ë”© í•¨ìˆ˜
def load_and_encode_image(image_path):
    with Image.open(image_path) as image:
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG")
        return base64.b64encode(buffered.getvalue()).decode()

# UI ì„¤ì •
image_str = load_and_encode_image(image_path=image_path)
st.markdown("""
    <style>
    .title {
        text-align: center;
        font-size: 50px;
    }
    </style>
    <div class="title">ğŸ’¬ Koreatech_GPT</div>
    """, unsafe_allow_html=True)
st.write("---")
img_html = f'<div style="text-align: center;"><img src="data:image/jpeg;base64,{image_str}" style="width:auto;"/></div>'
st.markdown(img_html, unsafe_allow_html=True)


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
persist_directory = 'db'
embedding = OpenAIEmbeddings()

def load_and_split_documents(file_path):
    try:
        # PDF íŒŒì¼ ë¡œë“œ
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader()
            documents = loader.load(file_path)
        
        # JSON íŒŒì¼ ë¡œë“œ
        elif file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as file:
                json_data = json.load(file)
                # JSON íŒŒì¼ì—ì„œ 'content' í‚¤ì˜ ê°’ì„ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                documents = [json_data['content']]
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
        else:
            loader = DirectoryLoader('./test', glob="*.txt", loader_cls=TextLoader)
            documents = loader.load()
        
        # ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        texts = text_splitter.split_documents(documents)
        return texts
    
    except Exception as e:
        st.error(f"ë¬¸ì„œ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

texts = load_and_split_documents('/Users/icarus/Library/CloudStorage/OneDrive-koreatech.ac.kr/Python/solo_study/project/grad_sup/test')  # ë¬¸ì„œ ê²½ë¡œ ì§€ì •

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„±
vectordb = Chroma.from_documents(
    documents=texts,
    embedding=embedding,
    persist_directory=persist_directory)

vectordb = Chroma(
    persist_directory = persist_directory, 
    embedding_function=embedding)

# ê²€ìƒ‰ ê¸°ëŠ¥ ì„¤ì •
retriever = vectordb.as_retriever(search_kwargs={"k":2})

prompt_template = """

Of course! The modified prompt template would be:

You are a knowledgeable and real-time AI chatbot assistant for Koreatech. Your primary role is to deliver precise and useful information about the university's educational programs, policies, campus life, and various services. It's crucial to formulate your responses to be thorough, well-researched, and considerate. If uncertain about an answer, candidly admit your lack of information and recommend contacting the relevant university department for further clarification. Remember, it's 2023 November 28th.

Now, with this context, answer the following query in Korean: {context}

Question: {question}

Answer in Korean:


"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
chain_type_kwargs = {"prompt": PROMPT}

llm = ChatOpenAI(
    temperature=1,              
    max_tokens=2048,            
    model_name='gpt-4-0613' 
)

# RetrievalQA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type="stuff", 
    retriever=retriever, 
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs,
)

def process_llm_response(llm_response):
    if isinstance(llm_response, dict) and "source_documents" in llm_response:
        sources = "\n".join([source.metadata['source'] for source in llm_response["source_documents"]])
        return f"{llm_response.get('result', 'ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}\n\nê´€ë ¨ ì†ŒìŠ¤:\n{sources}"
    else:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


st.header("Koreatech GPTì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!!")

if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content="You are a helpful AI assistant for Koreatech")
    ]
    st.session_state.messages.append(AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”, Koreatech-GPTì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"))

# ì‚¬ìš©ì ì…ë ¥ í•„ë“œ
if 'user_input' not in st.session_state:
    st.session_state['user_input'] = ''

user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ", key="user_input")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state.messages.append(HumanMessage(content=user_input))
    
    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        result = qa_chain({"query": user_input})
        response_t = process_llm_response(result)
    
    # ì±—ë´‡ ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state.messages.append(AIMessage(content=response_t))

# ëŒ€í™” ì´ë ¥ í‘œì‹œ
for i, msg in enumerate(st.session_state.messages):
    if isinstance(msg, HumanMessage):
        message(msg.content, is_user=True, key=f"message_{i}_user")
    elif isinstance(msg, AIMessage):
        message(msg.content, is_user=False, key=f"message_{i}_ai")